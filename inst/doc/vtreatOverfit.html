<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="John Mount, Nina Zumel" />

<meta name="date" content="2023-08-19" />

<title>vtreat overfit</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">vtreat overfit</h1>
<h4 class="author">John Mount, Nina Zumel</h4>
<h4 class="date">2023-08-19</h4>



<p>Example showing safe “best practice” use of the <a href="https://cran.r-project.org/package=vtreat">‘vtreat’</a> variable
preparation library. For more on <code>vtreat</code> see <a href="https://github.com/WinVector/vtreat">here</a>.</p>
<p>Below we generate an example data frame with no relation between x
and y. We are using a synthetic data set so we know what the “right
answer is” (no signal). False fitting on no-signal variables is bad for
several reasons:</p>
<ul>
<li>It creates undesirable biases in variable quality estimates and in
subsequent models.</li>
<li>It “hides degrees of freedom” from subsequent models.</li>
<li>It creates the false impression you have a good result (which you
may fail to falsify).</li>
<li>Complex bad variables can starve out simple weak good
variables.</li>
</ul>
<p>This example shows things we don’t want to happen, and then the
additional precautions that help prevent them.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">22626</span>)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">sample</span>(<span class="fu">paste</span>(<span class="st">&#39;level&#39;</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>,<span class="at">sep=</span><span class="st">&#39;&#39;</span>),<span class="dv">2000</span>,<span class="at">replace=</span><span class="cn">TRUE</span>)) <span class="co"># independent variable.</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>d<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="fu">nrow</span>(d))<span class="sc">&gt;</span><span class="fl">0.5</span>  <span class="co"># the quantity to be predicted, notice: independent of variables.</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>d<span class="sc">$</span>rgroup <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="dv">100</span><span class="sc">*</span><span class="fu">runif</span>(<span class="fu">nrow</span>(d)))  <span class="co"># the random group used for splitting the data set, not a variable.</span></span></code></pre></div>
<div id="bad-practice-using-the-same-data-to-treat-and-to-train" class="section level2">
<h2>Bad Practice: Using the same data to treat and to train</h2>
<p>Using the same set of data to prepare the variable encoding and train
the model can lead to the false belief (derived from the training set)
that the model fit well. This is largely due to the treated variable
appearing to consume only one degree of freedom, when it in fact
consumes many more. In many cases a reasonable setting of
<code>pruneSig</code> (say 0.01) will help against a noise variable
being considered desirable, but selected variables may still be mis-used
by downstream modeling.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>dTrain <span class="ot">&lt;-</span> d[d<span class="sc">$</span>rgroup<span class="sc">&lt;=</span><span class="dv">80</span>,,drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>dTest <span class="ot">&lt;-</span> d[d<span class="sc">$</span>rgroup<span class="sc">&gt;</span><span class="dv">80</span>,,drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&#39;vtreat&#39;</span>)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>treatments <span class="ot">&lt;-</span> vtreat<span class="sc">::</span><span class="fu">designTreatmentsC</span>(dTrain,<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="cn">TRUE</span>,</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>  <span class="at">rareCount=</span><span class="dv">0</span> <span class="co"># Note: usually want rareCount&gt;0, setting to zero to illustrate problem</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## [1] &quot;vtreat 1.6.4 inspecting inputs Sat Aug 19 12:10:21 2023&quot;
## [1] &quot;designing treatments Sat Aug 19 12:10:21 2023&quot;
## [1] &quot; have initial level statistics Sat Aug 19 12:10:21 2023&quot;
## [1] &quot; scoring treatments Sat Aug 19 12:10:21 2023&quot;
## [1] &quot;have treatment plan Sat Aug 19 12:10:21 2023&quot;
## [1] &quot;rescoring complex variables Sat Aug 19 12:10:21 2023&quot;
## [1] &quot;done rescoring complex variables Sat Aug 19 12:10:21 2023&quot;</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>dTrainTreated <span class="ot">&lt;-</span> vtreat<span class="sc">::</span><span class="fu">prepare</span>(treatments,dTrain,</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>  <span class="at">pruneSig=</span><span class="fu">c</span>() <span class="co"># Note: usually want pruneSig to be a small fraction, setting to null to illustrate problem</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Warning in prepare.treatmentplan(treatments, dTrain, pruneSig = c()): possibly
## called prepare() on same data frame as
## designTreatments*()/mkCrossFrame*Experiment(), this can lead to over-fit.  To
## avoid this, please use mkCrossFrame*Experiment$crossFrame.</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(y<span class="sc">~</span>x_catB,<span class="at">data=</span>dTrainTreated,<span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span><span class="st">&#39;logit&#39;</span>))</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(m1))  <span class="co"># notice low residual deviance</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x_catB, family = binomial(link = &quot;logit&quot;), 
##     data = dTrainTreated)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.03361    0.07170   0.469    0.639    
## x_catB       1.00624    0.11442   8.794   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2266.1  on 1634  degrees of freedom
## Residual deviance: 1131.2  on 1633  degrees of freedom
## AIC: 1135.2
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>dTrain<span class="sc">$</span>predM1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(m1,<span class="at">newdata=</span>dTrainTreated,<span class="at">type=</span><span class="st">&#39;response&#39;</span>)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co"># devtools::install_github(&quot;WinVector/WVPlots&quot;)</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co"># library(&#39;WVPlots&#39;)</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>plotRes <span class="ot">&lt;-</span> <span class="cf">function</span>(d,predName,yName,title) {</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>  <span class="fu">print</span>(title)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>  tab <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">truth=</span>d[[yName]],<span class="at">pred=</span>d[[predName]]<span class="sc">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>  <span class="fu">print</span>(tab)</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>  diag <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">vapply</span>(<span class="fu">seq_len</span>(<span class="fu">min</span>(<span class="fu">dim</span>(tab))),</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>                     <span class="cf">function</span>(i) tab[i,i],<span class="fu">numeric</span>(<span class="dv">1</span>)))</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>  acc <span class="ot">&lt;-</span> diag<span class="sc">/</span><span class="fu">sum</span>(tab)</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a><span class="co">#  if(requireNamespace(&quot;WVPlots&quot;,quietly=TRUE)) {</span></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a><span class="co">#     print(WVPlots::ROCPlot(d,predName,yName,title))</span></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a><span class="co">#  }</span></span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">&#39;accuracy&#39;</span>,acc))</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>}</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a><span class="co"># evaluate model on training</span></span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a><span class="fu">plotRes</span>(dTrain,<span class="st">&#39;predM1&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="st">&#39;model1 on train&#39;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;model1 on train&quot;
##        pred
## truth   FALSE TRUE
##   FALSE   556  248
##   TRUE     92  739
## [1] &quot;accuracy 0.792048929663609&quot;</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># evaluate model on test</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>dTestTreated <span class="ot">&lt;-</span> vtreat<span class="sc">::</span><span class="fu">prepare</span>(treatments,dTest,<span class="at">pruneSig=</span><span class="fu">c</span>())</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>dTest<span class="sc">$</span>predM1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(m1,<span class="at">newdata=</span>dTestTreated,<span class="at">type=</span><span class="st">&#39;response&#39;</span>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="fu">plotRes</span>(dTest,<span class="st">&#39;predM1&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="st">&#39;model1 on test&#39;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;model1 on test&quot;
##        pred
## truth   FALSE TRUE
##   FALSE    57  123
##   TRUE     65  120
## [1] &quot;accuracy 0.484931506849315&quot;</code></pre>
<p>The above is bad: we saw a “significant” model fit on training data
(even though there is no relation to be found). This means the treated
training data can be confusing to machine learning techniques and to the
analyst. The issue is that the training data is no longer exchangeable
with the test data because the training data was used to build the
variable encodings. One way to avoid this is to not use the training
data for variable encoding construction, but instead use a third set for
this task.</p>
<div id="what-went-wrong" class="section level3">
<h3>What went wrong?</h3>
<p>Notice that vtreat did not think there was any usable signal, and did
not want us to use the variables: the values in
<code>treatments$scoreFrame$sig</code> are all much larger than a
nominally acceptable significance level like 0.05. The variables stayed
in our model because we did not prune them (<em>ie</em> we set
<code>pruneSig=c()</code>). Also notice we set <code>rareCount=0</code>,
which allows the use of very rare levels (which help drive the
problem).</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">print</span>(treatments<span class="sc">$</span>scoreFrame)</span></code></pre></div>
<pre><code>##   varName varMoves          rsq        sig needsSplit extraModelDegrees
## 1  x_catP     TRUE 0.0001174834 0.60586892       TRUE               803
## 2  x_catB     TRUE 0.0020852084 0.02972052       TRUE               803
##   origName code default_threshold recommended
## 1        x catP               0.5       FALSE
## 2        x catB               0.5        TRUE</code></pre>
<p>Subsequently, the down-stream machine learning (in this case a
standard logistic regression) used the variable incorrectly. The
modeling algorithm gave the variable a non-negligible coefficient
(around 3) that it thought was reliably bounded away from zero; it also
believed that the resulting model almost halved deviance (when in fact
it explained nothing). So any variables that do get through may have
distributional issues (and misleadingly low apparent degrees of
freedom).</p>
<p><strong>Rare levels of a categorical variable</strong></p>
<p>The biggest contributors to this distributional issue tend to be rare
levels of categorical variables. Since the individual levels are rare we
have unreliable estimates for their effects, and if there are very many
of them we may see quite a large effect in aggregate. To help combat
this we have a control called <code>rareLevels</code>. Any level that is
observed no more than <code>rareLevels</code> times during training is
re-mapped to a new special level called <em>rare</em> and not allowed to
directly contribute (i.e. can not generate unique indicator columns, and
doesn’t have a direct effect on <code>catB</code> or <code>catN</code>
encodings). If all the rare levels have a distinct behavior after
grouping, the <em>rare</em> level can capture that.</p>
<p><strong>Impact-coding of categorical variables with many
levels</strong></p>
<p>Another undesirable effect is over-estimating significance of derived
variable fit for <code>catB</code> and <code>catN</code> impact-coded
variables. To fight this vtreat attempts to estimate out of sample or
cross-validated effect significances (when it has enough data). With
enough data, setting the <code>pruneSig</code> parameter during
<code>prepare()</code> will help remove noise variables. One can set
<code>pruneSig</code> to something like <em>1/number-of-columns</em> to
ensure that with high probability only an constant number of truly
useless variables make it to later modeling. However, the significance
of a given effect size for variables that actually have some signal
(i.e. non-noise variables) can still be sensitive to in/out sample
scoring and the hiding of degrees of freedom that occurs when a large
categorical variable (that represents a large number of degrees of
freedom) is re-coded as an impact or effect (which appears to have only
a single degree of freedom).</p>
<p>We next show how to avoid these undesirable illusory effects: better
practice in partitioning and using training data. We are doing more with
our data (essentially chaining models), so we have to take a bit more
care with our data.</p>
</div>
</div>
<div id="correct-practice-12-use-different-data-to-treat-and-train" class="section level2">
<h2>Correct Practice 1/2: Use different data to treat and train</h2>
<p>Below is part of our suggested work pattern: coding/train/test
split.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>dCode <span class="ot">&lt;-</span> d[d<span class="sc">$</span>rgroup<span class="sc">&lt;=</span><span class="dv">20</span>,,drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>dTrain <span class="ot">&lt;-</span> d[(d<span class="sc">$</span>rgroup<span class="sc">&gt;</span><span class="dv">20</span>) <span class="sc">&amp;</span> (d<span class="sc">$</span>rgroup<span class="sc">&lt;=</span><span class="dv">80</span>),,drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>treatments <span class="ot">&lt;-</span> vtreat<span class="sc">::</span><span class="fu">designTreatmentsC</span>(dCode,<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="cn">TRUE</span>,</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>                                        <span class="at">rareCount=</span><span class="dv">0</span>,  <span class="co"># Note set this to something larger, like 5</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>                                        <span class="at">rareSig=</span><span class="fu">c</span>() <span class="co"># Note set this to something like 0.3</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## [1] &quot;vtreat 1.6.4 inspecting inputs Sat Aug 19 12:10:21 2023&quot;
## [1] &quot;designing treatments Sat Aug 19 12:10:21 2023&quot;
## [1] &quot; have initial level statistics Sat Aug 19 12:10:21 2023&quot;
## [1] &quot; scoring treatments Sat Aug 19 12:10:21 2023&quot;
## [1] &quot;have treatment plan Sat Aug 19 12:10:21 2023&quot;
## [1] &quot;rescoring complex variables Sat Aug 19 12:10:21 2023&quot;
## [1] &quot;done rescoring complex variables Sat Aug 19 12:10:21 2023&quot;</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>dTrainTreated <span class="ot">&lt;-</span> vtreat<span class="sc">::</span><span class="fu">prepare</span>(treatments,dTrain,</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>                                 <span class="at">pruneSig=</span><span class="fu">c</span>() <span class="co"># Note: set this to filter, like 0.05 or 1/nvars</span></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>)</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(y<span class="sc">~</span>x_catB,<span class="at">data=</span>dTrainTreated,<span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span><span class="st">&#39;logit&#39;</span>))</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(m2)) <span class="co"># notice high residual deviance</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x_catB, family = binomial(link = &quot;logit&quot;), 
##     data = dTrainTreated)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)  0.01040    0.05746   0.181    0.856
## x_catB       0.01713    0.01054   1.625    0.104
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1687.1  on 1216  degrees of freedom
## Residual deviance: 1684.4  on 1215  degrees of freedom
## AIC: 1688.4
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>dTrain<span class="sc">$</span>predM2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(m2,<span class="at">newdata=</span>dTrainTreated,<span class="at">type=</span><span class="st">&#39;response&#39;</span>)</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="fu">plotRes</span>(dTrain,<span class="st">&#39;predM2&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="st">&#39;model2 on train&#39;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;model2 on train&quot;
##        pred
## truth   FALSE TRUE
##   FALSE   105  499
##   TRUE     92  521
## [1] &quot;accuracy 0.514379622021364&quot;</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># We do not advise creating dCodeTreated for any purpose other than</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="co"># diagnostic plotting.  You should not use the treated coding data</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="co"># for anything (as that would undo the benefit of having a separate</span></span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="co"># coding data subset).</span></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>dCodeTreated <span class="ot">&lt;-</span> vtreat<span class="sc">::</span><span class="fu">prepare</span>(treatments,dCode,<span class="at">pruneSig=</span><span class="fu">c</span>())</span></code></pre></div>
<pre><code>## Warning in prepare.treatmentplan(treatments, dCode, pruneSig = c()): possibly
## called prepare() on same data frame as
## designTreatments*()/mkCrossFrame*Experiment(), this can lead to over-fit.  To
## avoid this, please use mkCrossFrame*Experiment$crossFrame.</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>dCode<span class="sc">$</span>predM2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(m2,<span class="at">newdata=</span>dCodeTreated,<span class="at">type=</span><span class="st">&#39;response&#39;</span>)</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a><span class="fu">plotRes</span>(dCode,<span class="st">&#39;predM2&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="st">&#39;model2 on coding set&#39;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;model2 on coding set&quot;
##        pred
## truth   FALSE TRUE
##   FALSE   174   26
##   TRUE      3  215
## [1] &quot;accuracy 0.930622009569378&quot;</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>dTestTreated <span class="ot">&lt;-</span> vtreat<span class="sc">::</span><span class="fu">prepare</span>(treatments,dTest,<span class="at">pruneSig=</span><span class="fu">c</span>())</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>dTest<span class="sc">$</span>predM2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(m2,<span class="at">newdata=</span>dTestTreated,<span class="at">type=</span><span class="st">&#39;response&#39;</span>)</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a><span class="fu">plotRes</span>(dTest,<span class="st">&#39;predM2&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="st">&#39;model2 on test set&#39;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;model2 on test set&quot;
##        pred
## truth   FALSE TRUE
##   FALSE    26  154
##   TRUE     42  143
## [1] &quot;accuracy 0.463013698630137&quot;</code></pre>
<p>In the above example we saw training and test performance are similar
– and equally poor, as they should be since there is no signal. Though
it didn’t happen in this case, note the coding set can (falsely) show
high performance. This is the bad behavior we wanted to isolate out of
the training set.</p>
<p>Remember, the goal isn’t good performance on training- it is good
performance on future data (simulated by test). So doing well on
training and bad on test is worse than doing bad on both test and
training.</p>
<p>There are, of course, other methods to avoid the bias introduced in
using the same data to both treat/encode the variables and to train the
model. vtreat incorporates a number of these methods, including
smoothing (controlled through <code>smFactor</code>) and pruning of rare
levels (controlled through <code>rareSig</code>).</p>
</div>
<div id="correct-practice-22-use-simulated-out-of-sample-methods-cross-methods" class="section level2">
<h2>Correct Practice 2/2: Use simulated out of sample methods (cross
methods)</h2>
<p>Another effective technique: cross-constructed training frames can
also be accessed by using <code>mkCrossFrameCExperiment</code> or
<code>mkCrossFrameNExperiment</code>, which we demonstrate here.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>dTrain <span class="ot">&lt;-</span> d[d<span class="sc">$</span>rgroup<span class="sc">&lt;=</span><span class="dv">80</span>,,drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>xdat <span class="ot">&lt;-</span> vtreat<span class="sc">::</span><span class="fu">mkCrossFrameCExperiment</span>(dTrain,<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="cn">TRUE</span>,</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>                                  <span class="at">rareCount=</span><span class="dv">0</span>,  <span class="co"># Note set this to something larger, like 5</span></span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>                                  <span class="at">rareSig=</span><span class="fu">c</span>())</span></code></pre></div>
<pre><code>## [1] &quot;vtreat 1.6.4 start initial treatment design Sat Aug 19 12:10:21 2023&quot;
## [1] &quot; start cross frame work Sat Aug 19 12:10:21 2023&quot;
## [1] &quot; vtreat::mkCrossFrameCExperiment done Sat Aug 19 12:10:21 2023&quot;</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>treatments <span class="ot">&lt;-</span> xdat<span class="sc">$</span>treatments</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a><span class="fu">print</span>(treatments<span class="sc">$</span>scoreFrame)</span></code></pre></div>
<pre><code>##   varName varMoves          rsq          sig needsSplit extraModelDegrees
## 1  x_catP     TRUE 2.772417e-05 0.8020822863       TRUE               803
## 2  x_catB     TRUE 5.776072e-03 0.0002969686       TRUE               803
##   origName code default_threshold recommended
## 1        x catP               0.5       FALSE
## 2        x catB               0.5        TRUE</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>dTrainTreated <span class="ot">&lt;-</span> xdat<span class="sc">$</span>crossFrame</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">glm</span>(y<span class="sc">~</span>x_catB,<span class="at">data=</span>dTrainTreated,<span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span><span class="st">&#39;logit&#39;</span>))</span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(m3)) <span class="co"># notice high residual deviance</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x_catB, family = binomial(link = &quot;logit&quot;), 
##     data = dTrainTreated)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) 0.031450   0.049508   0.635    0.525
## x_catB      0.007853   0.007422   1.058    0.290
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2266.1  on 1634  degrees of freedom
## Residual deviance: 2265.0  on 1633  degrees of freedom
## AIC: 2269
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a>dTrainTreated<span class="sc">$</span>predM3 <span class="ot">&lt;-</span> <span class="fu">predict</span>(m3,<span class="at">newdata=</span>dTrainTreated,<span class="at">type=</span><span class="st">&#39;response&#39;</span>)</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a><span class="fu">plotRes</span>(dTrainTreated,<span class="st">&#39;predM3&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="st">&#39;model3 on train&#39;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;model3 on train&quot;
##        pred
## truth   FALSE TRUE
##   FALSE   184  620
##   TRUE    205  626
## [1] &quot;accuracy 0.495412844036697&quot;</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a>dTestTreated <span class="ot">&lt;-</span> vtreat<span class="sc">::</span><span class="fu">prepare</span>(treatments,dTest,<span class="at">pruneSig=</span><span class="fu">c</span>())</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a>dTest<span class="sc">$</span>predM3 <span class="ot">&lt;-</span> <span class="fu">predict</span>(m3,<span class="at">newdata=</span>dTestTreated,<span class="at">type=</span><span class="st">&#39;response&#39;</span>)</span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a><span class="fu">plotRes</span>(dTest,<span class="st">&#39;predM3&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="st">&#39;model3 on test set&#39;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;model3 on test set&quot;
##        pred
## truth   FALSE TRUE
##   FALSE    44  136
##   TRUE     53  132
## [1] &quot;accuracy 0.482191780821918&quot;</code></pre>
<p>Notice the glm significance is off, but the model quality is similar
on train and test, and the scoreFrame significance is a correct
indication.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
