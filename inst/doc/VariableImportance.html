<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="John Mount" />

<meta name="date" content="2023-08-19" />

<title>vtreat Variable Importance</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">vtreat Variable Importance</h1>
<h4 class="author">John Mount</h4>
<h4 class="date">2023-08-19</h4>



<p><a href="https://github.com/WinVector/vtreat"><code>vtreat</code></a>’s
purpose is to produce pure numeric <a href="https://www.r-project.org"><code>R</code></a>
<code>data.frame</code>s that are ready for <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised
predictive modeling</a> (predicting a value from other values). By ready
we mean: a purely numeric data frame with no missing values and a
reasonable number of columns (missing-values re-encoded with indicators,
and high-degree categorical re-encode by effects codes or impact
codes).</p>
<p>Part of the <code>vtreat</code> philosophy is to assume after the
<code>vtreat</code> variable processing the next step is a sophisticated
<a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised
machine learning</a> method. Under this assumption we assume the machine
learning methodology (be it regression, tree methods, random forests,
boosting, or neural nets) will handle issues of redundant variables,
joint distributions of variables, overall regularization, and joint
dimension reduction.</p>
<p>However, an important exception is: variable screening. In practice
we have seen wide data-warehouses with hundreds of columns overwhelm and
defeat state of the art machine learning algorithms due to over-fitting.
We have some synthetic examples of this (<a href="https://win-vector.com/2014/02/01/bad-bayes-an-example-of-why-you-need-hold-out-testing/">here</a>
and <a href="https://win-vector.com/talks-and-presentations/">here</a>).</p>
<p>The upshot is: even in 2018 you can not treat every column you find
in a data warehouse as a variable. You must at least perform some basic
screening.</p>
<p>To help with this <code>vtreat</code> incorporates a per-variable
linear significance report. This report shows how useful each variable
is taken alone in a linear or generalized linear model (some details can
be found <a href="https://arxiv.org/abs/1611.09477">here</a>). However,
this sort of calculation was optimized for speed, not discovery
power.</p>
<p><code>vtreat</code> now includes a direct variable valuation system
that works very well with complex numeric relationships. It is a
function called <a href="https://winvector.github.io/vtreat/reference/value_variables_N.html"><code>vtreat::value_variables_N()</code></a>
for numeric or regression problems and <a href="https://winvector.github.io/vtreat/reference/value_variables_C.html"><code>vtreat::value_variables_C()</code></a>
for binomial classification problems. It works by fitting two
transformed copies of each numeric variable to the outcome. One
transform is a low frequency transform realized as an optimal
<code>k</code>-segment linear model for a moderate choice of
<code>k</code>. The other fit is a high-frequency trasnform realized as
a <code>k</code>-nearest neighbor average for moderate choice of
<code>k</code>. Some of the methodology is shown <a href="https://github.com/WinVector/vtreat/blob/master/extras/SegFitter.md">here</a>.</p>
<p>We recommend using <code>vtreat::value_variables_*()</code> as an
initial variable screen.</p>
<p>Let’s demonstrate this using the data from the segment fitter
example. In our case the value to be predicted (“<code>y</code>”) is a
noisy copy of <code>sin(x)</code>. Let’s set up our example data:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1999</span>)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">15</span>, <span class="at">by =</span> <span class="fl">0.25</span>))</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>d<span class="sc">$</span>y_ideal <span class="ot">&lt;-</span> <span class="fu">sin</span>(d<span class="sc">$</span>x)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>d<span class="sc">$</span>x_noise <span class="ot">&lt;-</span> d<span class="sc">$</span>x[<span class="fu">sample.int</span>(<span class="fu">nrow</span>(d), <span class="fu">nrow</span>(d), <span class="at">replace =</span> <span class="cn">FALSE</span>)]</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>d<span class="sc">$</span>y <span class="ot">&lt;-</span> d<span class="sc">$</span>y_ideal <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">rnorm</span>(<span class="fu">nrow</span>(d))</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="fu">dim</span>(d)</span></code></pre></div>
<pre><code>## [1] 61  4</code></pre>
<p>Now a simple linear valuation of the the variables can be produced as
follows.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>cfe <span class="ot">&lt;-</span> vtreat<span class="sc">::</span><span class="fu">mkCrossFrameNExperiment</span>(</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>  d, </span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>  <span class="at">varlist =</span> <span class="fu">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;x_noise&quot;</span>), </span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>  <span class="at">outcomename =</span> <span class="st">&quot;y&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;vtreat 1.6.4 start initial treatment design Sat Aug 19 12:10:18 2023&quot;
## [1] &quot; start cross frame work Sat Aug 19 12:10:18 2023&quot;
## [1] &quot; vtreat::mkCrossFrameNExperiment done Sat Aug 19 12:10:18 2023&quot;</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>sf <span class="ot">&lt;-</span> cfe<span class="sc">$</span>treatments<span class="sc">$</span>scoreFrame</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(sf[, <span class="fu">c</span>(<span class="st">&quot;varName&quot;</span>, <span class="st">&quot;rsq&quot;</span>, <span class="st">&quot;sig&quot;</span>)])</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">varName</th>
<th align="right">rsq</th>
<th align="right">sig</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">x</td>
<td align="right">0.003136</td>
<td align="right">0.6681673</td>
</tr>
<tr class="even">
<td align="left">x_noise</td>
<td align="right">0.025578</td>
<td align="right">0.2182462</td>
</tr>
</tbody>
</table>
<p>Notice the signal carrying variable did not score better (having a
larger <code>r</code>-squared and a smaller (better) significance value)
than the noise variable (that is unrelated to the outcome). This is
because the relation between <code>x</code> and <code>y</code> is not
linear.</p>
<p>Now let’s try <code>vtreat::value_variables_N()</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>vf <span class="ot">=</span> vtreat<span class="sc">::</span><span class="fu">value_variables_N</span>(</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>  d, </span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>  <span class="at">varlist =</span> <span class="fu">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;x_noise&quot;</span>),</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>  <span class="at">outcomename =</span> <span class="st">&quot;y&quot;</span>)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(vf[, <span class="fu">c</span>(<span class="st">&quot;var&quot;</span>, <span class="st">&quot;rsq&quot;</span>, <span class="st">&quot;sig&quot;</span>)])</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">var</th>
<th align="right">rsq</th>
<th align="right">sig</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">x</td>
<td align="right">0.1999094</td>
<td align="right">0.0009099</td>
</tr>
<tr class="even">
<td align="left">x_noise</td>
<td align="right">0.0255780</td>
<td align="right">0.6547385</td>
</tr>
</tbody>
</table>
<p>Now the difference is night and day. The important variable
<code>x</code> is singled out (scores very well), and the unimportant
variable <code>x_noise</code> doesn’t often score well. Though, as with
all significance tests, useless variables can get lucky from time to
time- (an issue that can be addressed by using a <a href="https://win-vector.com/2017/09/08/remember-p-values-are-not-effect-sizes/">Cohen’s-<code>d</code>
style calculation</a>).</p>
<p>Our modeling advice is:</p>
<ul>
<li>Use <code>vtreat::value_variables_*()</code></li>
<li>Pick all variables with
<code>sig &lt;= 1/number_of_variables_being_considered</code>.</li>
</ul>
<p>The idea is: each “pure noise” (or purely useless) variable has a
significance that is distributed uniformly between zero and one. So the
expected number of useless variables that make it through the above
screening is
<code>number_of_useless_varaibles * P[useless_sig &lt;= 1/number_of_variables_being_considered]</code>.
This equals
<code>number_of_useless_varaibles * 1/number_of_variables_being_considered</code>.
As
<code>number_of_useless_varaibles &lt;= number_of_variables_being_considered</code>
we get this quantity is no more than one. So we expect a constant number
of useless variables to sneak through this filter. The hope is: this
should not be enough useless variables to overwhelm the next stage
supervised machine learning step.</p>
<p>Obviously there are situations where variable importance can not be
discovered without considering joint distributions. The most famous one
being “xor” where the concept to be learned is if an odd or even number
of indicator variables are zero or one (each such variable is individual
completely uninformative about the outcome until you have all of the
variables simultaneously). However, for practical problems you often
have that most variables have a higher marginal predictive power taken
alone than they have in the final joint model (as other, better,
variables consume some of common variables’ predictive power in the
joint model). With this in mind single variable screening often at least
gives an indication where to look.</p>
<p>In conclusion the <code>vtreat</code> package and
<code>vtreat::value_variables_*()</code> can be a valuable addition to
your supervised learning practice.</p>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
